{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccd3e225-58d0-49f9-b793-8e8428e1bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing recurrence step by step:\n",
      "h_0 = 0.0\n",
      "h_1 = tanh(0.8 * 0.0000 + 1.0) = tanh(1.0000) = 0.7616\n",
      "h_2 = tanh(0.8 * 0.7616 + 0.5) = tanh(1.1093) = 0.8038\n",
      "h_3 = tanh(0.8 * 0.8038 + -0.25) = tanh(0.3930) = 0.3740\n",
      "h_4 = tanh(0.8 * 0.3740 + 0.0) = tanh(0.2992) = 0.2906\n",
      "\n",
      "Final results: h_t for t = 1, ..., 4:\n",
      "hs = [0.7616 0.8038 0.374  0.2906]\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "import numpy as np\n",
    "\n",
    "# Recurrence: h_t = tanh(0.8 * h_{t-1} + x_t), h_0 = 0\n",
    "# Input sequence: x = [1.0, 0.5, -0.25, 0.0]\n",
    "\n",
    "x = np.array([1.0, 0.5, -0.25, 0.0])\n",
    "h = 0.0  # h_0 = 0\n",
    "hs = []\n",
    "\n",
    "print(\"Computing recurrence step by step:\")\n",
    "print(f\"h_0 = {h}\")\n",
    "\n",
    "for t, xt in enumerate(x, 1):\n",
    "    h_prev = h\n",
    "    h = np.tanh(0.8 * h + xt)\n",
    "    hs.append(h)\n",
    "    print(f\"h_{t} = tanh(0.8 * {h_prev:.4f} + {xt}) = tanh({0.8 * h_prev + xt:.4f}) = {h:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal results: h_t for t = 1, ..., 4:\")\n",
    "print(\"hs =\", np.round(hs, 4))\n",
    "\n",
    "# This is an unrolled graph because we expand the single recurrent operation across multiple time steps turning the temporal loop into a sequence of computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb39771-a197-4748-b539-3bc10de8fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=1: a_t=[1. 0.], h_t=[0.76159416 0.        ], o_t=[ 0.76159416 -0.76159416]\n",
      "t=1: y_t=[0.8210075 0.1789925], sum=1.000000\n",
      "t=2: y_t=[0.86822576 0.13177424]\n",
      "t=3: y_t=[0.87208178 0.12791822]\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "def rnn_step(h_prev, x_t, p):\n",
    "    # compute a_t\n",
    "    a_t = p['b'] + np.dot(p['W'], h_prev) + np.dot(p['U'], x_t)\n",
    "    # compute h_t\n",
    "    h_t = np.tanh(a_t)\n",
    "    # compute o_t\n",
    "    o_t = p['c'] + np.dot(p['V'], h_t)\n",
    "    # apply softmax\n",
    "    o_t_shifted = o_t - np.max(o_t)\n",
    "    y_t = np.exp(o_t_shifted) / np.sum(np.exp(o_t_shifted))\n",
    "    return h_t, y_t\n",
    "\n",
    "# Parameters\n",
    "p = {'b': np.array([1, 0]), 'W': np.array([[1, 1], [0, 1]]), \n",
    "     'U': np.array([[1, 1], [-1, 1]]), 'V': np.array([[1, -1], [-1, 1]]), \n",
    "     'c': np.array([0, 0])}\n",
    "\n",
    "# Run 3 steps\n",
    "h = np.array([0.0, 0.0])\n",
    "x = np.array([0.0, 0.0])\n",
    "\n",
    "for t in range(1, 4):\n",
    "    h, y = rnn_step(h, x, p)\n",
    "    if t == 1:\n",
    "        a_1 = p['b'] + np.dot(p['W'], np.array([0.0, 0.0])) + np.dot(p['U'], x)\n",
    "        h_1 = np.tanh(a_1)\n",
    "        o_1 = p['c'] + np.dot(p['V'], h_1)\n",
    "        print(f\"t=1: a_t={a_1}, h_t={h_1}, o_t={o_1}\")\n",
    "        print(f\"t=1: y_t={y}, sum={np.sum(y):.6f}\")\n",
    "    else:\n",
    "        print(f\"t={t}: y_t={y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52dad6bb-1160-4446-8731-e1ed8f43f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g = [3. 4.], norm = 5.0000\n",
      "clipped = [1.2 1.6], norm = 2.0000\n",
      "g = [0.1 0.2], norm = 0.2236\n",
      "clipped = [0.1 0.2], norm = 0.2236\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "def clip_grad(g, v):\n",
    "    norm_g = np.linalg.norm(g)\n",
    "    if norm_g == 0 or norm_g <= v:\n",
    "        return g\n",
    "    return v * g / norm_g\n",
    "\n",
    "# Test 1: g = [3, 4], v = 2.0\n",
    "g1 = np.array([3.0, 4.0])\n",
    "v1 = 2.0\n",
    "g1_clipped = clip_grad(g1, v1)\n",
    "print(f\"g = {g1}, norm = {np.linalg.norm(g1):.4f}\")\n",
    "print(f\"clipped = {g1_clipped}, norm = {np.linalg.norm(g1_clipped):.4f}\")\n",
    "\n",
    "# Test 2: Small vector\n",
    "g2 = np.array([0.1, 0.2])\n",
    "g2_clipped = clip_grad(g2, v1)\n",
    "print(f\"g = {g2}, norm = {np.linalg.norm(g2):.4f}\")\n",
    "print(f\"clipped = {g2_clipped}, norm = {np.linalg.norm(g2_clipped):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58dfdc4c-e7ed-4050-957b-6ba7d19a7161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task : Forget Factor Experiment\n",
      "Final values s_20:\n",
      "f = 0.2: s_20 = 1.000000\n",
      "f = 0.8: s_20 = 0.988471\n",
      "f = 0.95: s_20 = 0.641514\n"
     ]
    }
   ],
   "source": [
    "#Question 4\n",
    "# Forget factor experiment\n",
    "# s_t = f * s_{t-1} + (1-f) * 1, s_0 = 0\n",
    "\n",
    "forget_factors = [0.2, 0.8, 0.95]\n",
    "\n",
    "print(\"Task : Forget Factor Experiment\")\n",
    "print(\"Final values s_20:\")\n",
    "\n",
    "for f in forget_factors:\n",
    "    s = 0.0  # s_0 = 0\n",
    "    for t in range(1, 21):  # t = 1 to 20\n",
    "        s = f * s + (1 - f) * 1\n",
    "    print(f\"f = {f}: s_20 = {s:.6f}\")\n",
    "    #interpretation: f = 0.95: s_20 = 0.641514 retains information best because of its high forget factor which preserves more of the previous state, hence the key principle behind LSTM forget gates.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
